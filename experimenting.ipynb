{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80aee85c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30329b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import sys, copy, csv\n",
    "\n",
    "# external packages\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# local packages\n",
    "from utils import add_ids \n",
    "from class_split_data_loader import ClassSplitDataLoader\n",
    "from autoencoder import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b591d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup BigQuery upload\n",
    "from BQuploader import BQuploader\n",
    "BQ_project_id = 'vertical-federated-learning'\n",
    "dataset_id = 'experiment_data'\n",
    "bq_uploader = BQuploader(BQ_project_id, dataset_id)\n",
    "# To upload a local file:\n",
    "# bq_uploader.load_local_file_to_table(file_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b253f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3206f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'criterion' : nn.MSELoss(),\n",
    "    'epochs' : 20,\n",
    "    'n_encoders' : 3,\n",
    "    'input_size' : 784,\n",
    "    'hidden_sizes_encoder' : [512, 256],\n",
    "    'hidden_sizes_decoder' : [256, 512],\n",
    "    'encoded_size' : 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa28363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data_train = add_ids(MNIST)(\".\", download=True, transform=ToTensor())\n",
    "data_test  = add_ids(MNIST)(\".\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ae28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and create dataloaders\n",
    "dataloaders = []\n",
    "dataloaders_test = []\n",
    "for k in range(settings['n_encoders']):\n",
    "    dataloader = ClassSplitDataLoader(data_train, class_to_keep=k, remove_data=False, keep_order=True, batch_size=128) \n",
    "    dataloaders.append(dataloader)\n",
    "    dataloader = ClassSplitDataLoader(data_test, class_to_keep=k, remove_data=False, keep_order=True, batch_size=128) \n",
    "    dataloaders_test.append(dataloader)\n",
    "    # partition_dataset uses by default \"remove_data=True, keep_order=False\"\n",
    "\n",
    "# and add them to the settings\n",
    "settings['dataloaders'] = dataloaders\n",
    "settings['dataloaders_test'] = dataloaders_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76832091",
   "metadata": {},
   "source": [
    "## Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af41b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Autoencoder(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7229f5",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a506a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net):\n",
    "    train_perf, test_perf = [], []\n",
    "    for i in range(settings['epochs']):\n",
    "\n",
    "        running_loss = np.zeros(settings['n_encoders'])\n",
    "        running_test_MSE = np.zeros(settings['n_encoders'])\n",
    "\n",
    "        for k in range(settings['n_encoders']):\n",
    "            loss_train, loss_test = net.iter_training_one_encoder(k)\n",
    "            running_loss[k] += loss_train\n",
    "            running_test_MSE[k] += loss_test\n",
    "\n",
    "        print(f\"Epoch {i}/{settings['epochs']}\"\n",
    "                +f\" - Training loss: {np.average(running_loss)/settings['n_encoders']:.4f}\"\n",
    "                +f\" - testing MSE: {np.average(running_test_MSE)/settings['n_encoders']:.4f}\")\n",
    "        train_perf.append(running_loss/settings['n_encoders'])\n",
    "        test_perf.append(running_test_MSE/settings['n_encoders'])\n",
    "    return train_perf, test_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8292a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20 - Training loss: 0.1447 - testing MSE: 0.1413\n",
      "Epoch 1/20 - Training loss: 0.1411 - testing MSE: 0.1397\n",
      "Epoch 2/20 - Training loss: 0.1398 - testing MSE: 0.1386\n",
      "Epoch 3/20 - Training loss: 0.1388 - testing MSE: 0.1379\n",
      "Epoch 4/20 - Training loss: 0.1383 - testing MSE: 0.1375\n",
      "Epoch 5/20 - Training loss: 0.1379 - testing MSE: 0.1373\n",
      "Epoch 6/20 - Training loss: 0.1378 - testing MSE: 0.1371\n",
      "Epoch 7/20 - Training loss: 0.1376 - testing MSE: 0.1369\n",
      "Epoch 8/20 - Training loss: 0.1374 - testing MSE: 0.1368\n",
      "Epoch 9/20 - Training loss: 0.1373 - testing MSE: 0.1367\n",
      "Epoch 10/20 - Training loss: 0.1372 - testing MSE: 0.1366\n",
      "Epoch 11/20 - Training loss: 0.1371 - testing MSE: 0.1365\n",
      "Epoch 12/20 - Training loss: 0.1370 - testing MSE: 0.1365\n",
      "Epoch 13/20 - Training loss: 0.1370 - testing MSE: 0.1364\n",
      "Epoch 14/20 - Training loss: 0.1369 - testing MSE: 0.1364\n",
      "Epoch 15/20 - Training loss: 0.1369 - testing MSE: 0.1364\n",
      "Epoch 16/20 - Training loss: 0.1369 - testing MSE: 0.1363\n",
      "Epoch 17/20 - Training loss: 0.1368 - testing MSE: 0.1363\n",
      "Epoch 18/20 - Training loss: 0.1368 - testing MSE: 0.1362\n",
      "Epoch 19/20 - Training loss: 0.1367 - testing MSE: 0.1362\n",
      "Epoch 0/20 - Training loss: 0.1367 - testing MSE: 0.1362\n",
      "Epoch 1/20 - Training loss: 0.1367 - testing MSE: 0.1361\n",
      "Epoch 2/20 - Training loss: 0.1366 - testing MSE: 0.1361\n",
      "Epoch 3/20 - Training loss: 0.1366 - testing MSE: 0.1361\n",
      "Epoch 4/20 - Training loss: 0.1366 - testing MSE: 0.1361\n",
      "Epoch 5/20 - Training loss: 0.1366 - testing MSE: 0.1361\n",
      "Epoch 6/20 - Training loss: 0.1365 - testing MSE: 0.1360\n",
      "Epoch 7/20 - Training loss: 0.1365 - testing MSE: 0.1360\n",
      "Epoch 8/20 - Training loss: 0.1365 - testing MSE: 0.1360\n",
      "Epoch 9/20 - Training loss: 0.1365 - testing MSE: 0.1360\n",
      "Epoch 10/20 - Training loss: 0.1365 - testing MSE: 0.1360\n",
      "Epoch 11/20 - Training loss: 0.1365 - testing MSE: 0.1360\n",
      "Epoch 12/20 - Training loss: 0.1364 - testing MSE: 0.1360\n",
      "Epoch 13/20 - Training loss: 0.1364 - testing MSE: 0.1360\n",
      "Epoch 14/20 - Training loss: 0.1364 - testing MSE: 0.1359\n",
      "Epoch 15/20 - Training loss: 0.1364 - testing MSE: 0.1359\n",
      "Epoch 16/20 - Training loss: 0.1363 - testing MSE: 0.1359\n",
      "Epoch 17/20 - Training loss: 0.1363 - testing MSE: 0.1359\n",
      "Epoch 18/20 - Training loss: 0.1363 - testing MSE: 0.1358\n",
      "Epoch 19/20 - Training loss: 0.1363 - testing MSE: 0.1358\n",
      "Epoch 0/20 - Training loss: 0.1362 - testing MSE: 0.1358\n",
      "Epoch 1/20 - Training loss: 0.1362 - testing MSE: 0.1358\n",
      "Epoch 2/20 - Training loss: 0.1362 - testing MSE: 0.1358\n",
      "Epoch 3/20 - Training loss: 0.1362 - testing MSE: 0.1357\n",
      "Epoch 4/20 - Training loss: 0.1362 - testing MSE: 0.1357\n",
      "Epoch 5/20 - Training loss: 0.1361 - testing MSE: 0.1357\n",
      "Epoch 6/20 - Training loss: 0.1361 - testing MSE: 0.1357\n",
      "Epoch 7/20 - Training loss: 0.1361 - testing MSE: 0.1356\n",
      "Epoch 8/20 - Training loss: 0.1361 - testing MSE: 0.1356\n",
      "Epoch 9/20 - Training loss: 0.1360 - testing MSE: 0.1356\n",
      "Epoch 10/20 - Training loss: 0.1360 - testing MSE: 0.1356\n",
      "Epoch 11/20 - Training loss: 0.1360 - testing MSE: 0.1356\n",
      "Epoch 12/20 - Training loss: 0.1360 - testing MSE: 0.1356\n",
      "Epoch 13/20 - Training loss: 0.1360 - testing MSE: 0.1356\n",
      "Epoch 14/20 - Training loss: 0.1360 - testing MSE: 0.1355\n",
      "Epoch 15/20 - Training loss: 0.1360 - testing MSE: 0.1355\n",
      "Epoch 16/20 - Training loss: 0.1360 - testing MSE: 0.1355\n",
      "Epoch 17/20 - Training loss: 0.1360 - testing MSE: 0.1355\n",
      "Epoch 18/20 - Training loss: 0.1359 - testing MSE: 0.1355\n",
      "Epoch 19/20 - Training loss: 0.1359 - testing MSE: 0.1355\n",
      "Epoch 0/20 - Training loss: 0.1359 - testing MSE: 0.1355\n",
      "Epoch 1/20 - Training loss: 0.1359 - testing MSE: 0.1355\n",
      "Epoch 2/20 - Training loss: 0.1359 - testing MSE: 0.1355\n",
      "Epoch 3/20 - Training loss: 0.1359 - testing MSE: 0.1355\n",
      "Epoch 4/20 - Training loss: 0.1359 - testing MSE: 0.1355\n",
      "Epoch 5/20 - Training loss: 0.1359 - testing MSE: 0.1354\n",
      "Epoch 6/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 7/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 8/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 9/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 10/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 11/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 12/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 13/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 14/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 15/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 16/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 17/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 18/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 19/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 0/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 1/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 2/20 - Training loss: 0.1358 - testing MSE: 0.1354\n",
      "Epoch 3/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 4/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 5/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 6/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 7/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 8/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 9/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 10/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 11/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 12/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 13/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 14/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 15/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 16/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 17/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 18/20 - Training loss: 0.1357 - testing MSE: 0.1354\n",
      "Epoch 19/20 - Training loss: 0.1357 - testing MSE: 0.1354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:While loading to the table, errors were found : There are no column descriptions provided for table autoencoder_1_11a47d97_d002_4724_bf23_091c730a8fd7_source\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Upload failed for table autoencoder_1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Docs/perso/vertical-federated-learning/BQuploader.py:30\u001b[0m, in \u001b[0;36mBQuploader.load_local_file_to_table\u001b[0;34m(self, file_name, table_name)\u001b[0m\n\u001b[1;32m     29\u001b[0m     load_job  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBQclient\u001b[38;5;241m.\u001b[39mload_table_from_file(file_object, table_ref, job_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBQjob_config)\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mload_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Docs/perso/vfl_env/lib/python3.10/site-packages/google/cloud/bigquery/job/base.py:728\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    727\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m\"\u001b[39m: retry}\n\u001b[0;32m--> 728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Docs/perso/vfl_env/lib/python3.10/site-packages/google/api_core/future/polling.py:137\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 There are no column descriptions provided for table autoencoder_1_11a47d97_d002_4724_bf23_091c730a8fd7_source",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_encoders):\n\u001b[1;32m     32\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterow(BQsettings \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, k, train_perf[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][k]])\n\u001b[0;32m---> 34\u001b[0m \u001b[43mbq_uploader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_local_file_to_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Docs/perso/vertical-federated-learning/BQuploader.py:34\u001b[0m, in \u001b[0;36mBQuploader.load_local_file_to_table\u001b[0;34m(self, file_name, table_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m load_job\u001b[38;5;241m.\u001b[39merrors:\n\u001b[1;32m     33\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile loading to the table, errors were found : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpload failed for table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Upload failed for table autoencoder_1"
     ]
    }
   ],
   "source": [
    "# BQ table\n",
    "table_name = 'autoencoder_1'\n",
    "file_name = 'temp.csv'\n",
    "\n",
    "repeats = 5\n",
    "for n_encoders in [1, 3, 10]:\n",
    "    for encoded_size in [128, 56, 10]:\n",
    "        with open(file_name,'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['criterion','n_encoders','epochs','encoded_size','train_or_test','class','MSE'])\n",
    "\n",
    "            settings['n_encoders'] = n_encoders\n",
    "            settings['encoded_size'] = encoded_size\n",
    "            settings['dataloaders'] = dataloaders\n",
    "            settings['dataloaders_test'] = dataloaders_test\n",
    "            net = Autoencoder(settings)\n",
    "            BQsettings = [str(settings[x]) for x in ['criterion','n_encoders','epochs','encoded_size']]\n",
    "            for i in range(repeats):\n",
    "                train_perf, test_perf = train_network(net)\n",
    "                for k in range(n_encoders):\n",
    "                    writer.writerow(BQsettings + ['train', k, train_perf[-1][k]])\n",
    "                    writer.writerow(BQsettings + ['test', k, test_perf[-1][k]])\n",
    "\n",
    "        bq_uploader.load_local_file_to_table(file_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b912ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "train_perf, test_perf = np.array(train_perf), np.array(test_perf)\n",
    "x = range(settings['epochs'])\n",
    "for k in range(settings['n_encoders']):\n",
    "    ax.plot(x, train_perf[:,k], label=f'training data,  encoder {k}')\n",
    "    ax.plot(x, test_perf[:,k], label=f'testing data, encoder {k}')\n",
    "\n",
    "plt.title(\"Learning curves\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vfl_env",
   "language": "python",
   "name": "vfl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
