{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723c5053",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217ae81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import sys, copy\n",
    "\n",
    "# external packages\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import syft as sy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# local packages\n",
    "from utils import add_ids \n",
    "from data_loader import VerticalDataLoader\n",
    "from class_split_data_loader import ClassSplitDataLoader\n",
    "from shared_NN import SharedNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef88b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize important variables\n",
    "#hook = sy.TorchHook(torch)\n",
    "torch.manual_seed(0)\n",
    "n_encoders = 1 #number of encoders we will train\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec01dcd1",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f19b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = add_ids(MNIST)(\".\", download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445fc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and create dataloaders\n",
    "dataloaders = []\n",
    "for k in range(n_encoders):\n",
    "    dataloader = ClassSplitDataLoader(data, class_to_keep=k, remove_data=False, keep_order=True, batch_size=128) \n",
    "    dataloaders.append(dataloader)\n",
    "    # partition_dataset uses by default \"remove_data=True, keep_order=False\"\n",
    "    # Do not do this for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c58d83",
   "metadata": {},
   "source": [
    "### Create networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09304998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [512, 256]\n",
    "encoded_size = 128\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[1], encoded_size),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "decoder = nn.Sequential(\n",
    "        nn.Linear(encoded_size, hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[1], hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[0], input_size),\n",
    "        #nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "models = [copy.deepcopy(encoder) for k in range(n_encoders)] + [decoder]\n",
    "\n",
    "# Create optimisers for each segment and link to them\n",
    "optimizers = [optim.Adam(model.parameters(), lr=1e-3,) for model in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8bc0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define training functions\n",
    "def forward(encoder_index, input_vector):\n",
    "    encoder_output = models[encoder_index](input_vector)\n",
    "    encoded_vector = encoder_output.clone().detach()\n",
    "    encoded_vector = encoded_vector.requires_grad_()\n",
    "    decoder = models[-1]\n",
    "    return encoder_output, encoded_vector, decoder(encoded_vector)\n",
    "\n",
    "def backward(encoder_output, encoded_vector):\n",
    "    grads = encoded_vector.grad.clone().detach()\n",
    "    encoder_output.backward(grads)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39ca89",
   "metadata": {},
   "source": [
    "### Train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9761202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.070 - MSE (normalised): 0.070\n",
      "Epoch 1 - Training loss: 0.046 - MSE (normalised): 0.046\n",
      "Epoch 2 - Training loss: 0.035 - MSE (normalised): 0.035\n",
      "Epoch 3 - Training loss: 0.030 - MSE (normalised): 0.030\n",
      "Epoch 4 - Training loss: 0.026 - MSE (normalised): 0.026\n",
      "Epoch 5 - Training loss: 0.024 - MSE (normalised): 0.024\n",
      "Epoch 6 - Training loss: 0.022 - MSE (normalised): 0.022\n",
      "Epoch 7 - Training loss: 0.020 - MSE (normalised): 0.020\n",
      "Epoch 8 - Training loss: 0.018 - MSE (normalised): 0.018\n",
      "Epoch 9 - Training loss: 0.017 - MSE (normalised): 0.017\n",
      "Epoch 10 - Training loss: 0.016 - MSE (normalised): 0.016\n",
      "Epoch 11 - Training loss: 0.015 - MSE (normalised): 0.015\n",
      "Epoch 12 - Training loss: 0.015 - MSE (normalised): 0.015\n",
      "Epoch 13 - Training loss: 0.014 - MSE (normalised): 0.014\n",
      "Epoch 14 - Training loss: 0.014 - MSE (normalised): 0.014\n",
      "Epoch 15 - Training loss: 0.013 - MSE (normalised): 0.013\n",
      "Epoch 16 - Training loss: 0.013 - MSE (normalised): 0.013\n",
      "Epoch 17 - Training loss: 0.013 - MSE (normalised): 0.013\n",
      "Epoch 18 - Training loss: 0.013 - MSE (normalised): 0.013\n",
      "Epoch 19 - Training loss: 0.012 - MSE (normalised): 0.012\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_MSE = 0\n",
    "    \n",
    "    for k in range(n_encoders):\n",
    "        # for now, train the encoders one after another\n",
    "        dataloader = dataloaders[k]\n",
    "        \n",
    "        for ((data, ids),) in dataloader:\n",
    "            # Train a model\n",
    "            data = data.view(data.shape[0], -1)\n",
    "            data_for_comparison = copy.deepcopy(data)\n",
    "            \n",
    "            #1) Zero our grads\n",
    "            for opt in optimizers:\n",
    "                opt.zero_grad()\n",
    "            \n",
    "            #2) Make a prediction and move it to the encoder\n",
    "            encoder_output, encoded_vector, pred = forward(k, data)\n",
    "            \n",
    "            #3) Figure out how much we missed by\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(pred, data)\n",
    "            \n",
    "            #4) Backprop the loss on the end layer\n",
    "            loss.backward()\n",
    "            \n",
    "            #5) Feed Gradients backward through the nework\n",
    "            backward(encoder_output, encoded_vector)\n",
    "            \n",
    "            #6) Change the weights\n",
    "            for opt in optimizers:\n",
    "                opt.step()\n",
    "\n",
    "            # Collect statistics\n",
    "            running_loss += loss.item()\n",
    "            #accuracy for an autoencoder is the distance between data and pred\n",
    "            #loss = nn.MSELoss(reduction='none')\n",
    "            running_MSE += nn.MSELoss()(pred,data)\n",
    "    print(f\"Epoch {i} - Training loss: {running_loss/len(dataloader)/n_encoders:.3f}\"+\n",
    "            f\" - MSE (normalised): {running_MSE/len(dataloader)/n_encoders:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d704178",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32af9433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating encoder no: 0\n",
      "MSE sum: 449983808.000\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "data_test = add_ids(MNIST)(\".\", train=False, download=True, transform=ToTensor())\n",
    "dataloaders_test = []\n",
    "for k in range(n_encoders):\n",
    "    dataloader = ClassSplitDataLoader(data_test, class_to_keep=k, remove_data=False, keep_order=True, batch_size=128) \n",
    "    dataloaders_test.append(dataloader)\n",
    "\n",
    "# run validation\n",
    "running_loss = 0\n",
    "running_MSE = 0\n",
    "\n",
    "for k in range(n_encoders):\n",
    "    # for now, train the encoders one after another\n",
    "    dataloader = dataloaders_test[k]\n",
    "    print(f\"Evaluating encoder no: {k}\")\n",
    "    for epoch in range(100):\n",
    "        for ((data_batch, ids_batch),) in dataloader:\n",
    "            # Train a model\n",
    "            data_batch = data_batch.view(data_batch.shape[0], -1)\n",
    "            encoder_output = models[k](data_batch)\n",
    "            network_output = models[-1](encoder_output)\n",
    "            running_MSE += nn.MSELoss(reduction='sum')(data_batch,network_output)\n",
    "    print(f\"MSE: {running_MSE/len(dataloader)/n_encoders/input_size:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vfl_env",
   "language": "python",
   "name": "vfl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
