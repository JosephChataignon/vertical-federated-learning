{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723c5053",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217ae81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import sys, copy\n",
    "\n",
    "# external packages\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import syft as sy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# local packages\n",
    "from utils import add_ids \n",
    "from data_loader import VerticalDataLoader\n",
    "from class_split_data_loader import ClassSplitDataLoader\n",
    "from shared_NN import SharedNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef88b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize important variables\n",
    "#hook = sy.TorchHook(torch)\n",
    "torch.manual_seed(0)\n",
    "n_encoders = 3 #number of encoders we will train\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec01dcd1",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f19b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = add_ids(MNIST)(\".\", download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445fc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and create dataloaders\n",
    "dataloaders = []\n",
    "for k in range(n_encoders):\n",
    "    dataloader = ClassSplitDataLoader(data, class_to_keep=k, remove_data=False, keep_order=True, batch_size=128) \n",
    "    dataloaders.append(dataloader)\n",
    "    # partition_dataset uses by default \"remove_data=True, keep_order=False\"\n",
    "    # Do not do this for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c58d83",
   "metadata": {},
   "source": [
    "### Create networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09304998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 640]\n",
    "encoded_size = 10\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[1], encoded_size),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "decoder = nn.Sequential(\n",
    "        nn.Linear(encoded_size, hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[1], hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[0], input_size),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "models = [copy.deepcopy(encoder) for k in range(n_encoders)] + [decoder]\n",
    "\n",
    "# Create optimisers for each segment and link to them\n",
    "optimizers = [optim.SGD(model.parameters(), lr=0.03,) for model in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b142fddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'syft' has no attribute 'VirtualWorker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create some workers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_locations \u001b[38;5;241m=\u001b[39m [sy\u001b[38;5;241m.\u001b[39mVirtualWorker(hook, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_encoders)]\n\u001b[1;32m      3\u001b[0m model_locations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [sy\u001b[38;5;241m.\u001b[39mVirtualWorker(hook, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Send Model Segments to model locations\u001b[39;00m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create some workers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_locations \u001b[38;5;241m=\u001b[39m [\u001b[43msy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVirtualWorker\u001b[49m(hook, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_encoders)]\n\u001b[1;32m      3\u001b[0m model_locations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [sy\u001b[38;5;241m.\u001b[39mVirtualWorker(hook, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Send Model Segments to model locations\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'syft' has no attribute 'VirtualWorker'"
     ]
    }
   ],
   "source": [
    "# create some workers\n",
    "model_locations = [sy.VirtualWorker(hook, id=f\"encoder_{k}\") for k in range(n_encoders)]\n",
    "model_locations += [sy.VirtualWorker(hook, id=\"decoder\")]\n",
    "\n",
    "# Send Model Segments to model locations\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ccdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SharedNN\n",
    "sharedNN = SharedNN(models, optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Learning\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for k in range(n_encoders):\n",
    "        # for now, train the encoders one after another\n",
    "        dataloader = dataloaders[k]\n",
    "        \n",
    "        for ((data, ids),) in dataloader:\n",
    "            # Train a model\n",
    "            data = data.view(data.shape[0], -1)\n",
    "            data_for_comparison = copy.deepcopy(data)\n",
    "            # we need a copy of the data to compare to the output of the decoder\n",
    "            data = data.send(models[k].location)\n",
    "            data_for_comparison = data_for_comparison.send(models[k].location)\n",
    "\n",
    "            #1) Zero our grads\n",
    "            sharedNN.zero_grads()\n",
    "            \n",
    "            #2) Make a prediction and move it to the encoder\n",
    "            pred = sharedNN.forward(k, data)\n",
    "            pred = pred.move(models[k].location)\n",
    "            \n",
    "            #3) Figure out how much we missed by\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(pred, data)\n",
    "            \n",
    "            #4) Backprop the loss on the end layer\n",
    "            loss = loss.move(models[-1].location)\n",
    "            print(f'loss grad: {loss.copy().get().grad}')\n",
    "            loss.backward()\n",
    "            \n",
    "            #5) Feed Gradients backward through the nework\n",
    "            sharedNN.backward()\n",
    "            \n",
    "            #6) Change the weights\n",
    "            sharedNN.step()\n",
    "\n",
    "    # Collect statistics\n",
    "    running_loss += loss.get()\n",
    "    #correct_preds += pred.max(1)[1].eq(labels).sum().get().item()\n",
    "    total_preds += pred.get().size(0)\n",
    "\n",
    "    print(f\"Epoch {i} - Training loss: {running_loss/len(dataloader):.3f} - Accuracy: {100*correct_preds/total_preds:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vfl_env",
   "language": "python",
   "name": "vfl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
